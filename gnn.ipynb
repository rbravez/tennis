{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce90b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "994c4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_gnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce6eedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['tourney_date'].astype(str))\n",
    "df.sort_values('date').reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9b584c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_val  = np.datetime64('2019-01-01')\n",
    "cut_test = np.datetime64('2020-01-01')\n",
    "idx_train = np.where(df['date'].values <  cut_val)[0]\n",
    "idx_val   = np.where((df['date'].values >= cut_val) & (df['date'].values < cut_test))[0]\n",
    "idx_test  = np.where(df['date'].values >= cut_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de07641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target'].astype('int32').values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e51b3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['surface', 'tourney_level', 'player_A_hand', 'player_B_hand']:\n",
    "    df[col] = df[col].fillna('UNK').astype('category')\n",
    "surf_ohe   = pd.get_dummies(df['surface'], prefix='surf', drop_first=False)\n",
    "level_ohe  = pd.get_dummies(df['tourney_level'], prefix='lvl', drop_first=False)\n",
    "Ahand_ohe  = pd.get_dummies(df['player_A_hand'], prefix='Ah', drop_first=False)\n",
    "Bhand_ohe  = pd.get_dummies(df['player_B_hand'], prefix='Bh', drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8120a205",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['player_A_ht','player_B_ht','player_A_rank','player_B_rank',\n",
    "            'player_A_rank_points','player_B_rank_points',\n",
    "            'A_Glicko1_Rating','B_Glicko1_Rating',\n",
    "            'A_Glicko1_Surface_Rating','B_Glicko1_Surface_Rating',\n",
    "            'A_H2H_Streak','B_H2H_Streak',\n",
    "            'A_H2H_LevelWeighted_Wins','B_H2H_LevelWeighted_Wins',\n",
    "            'A_Age30','B_Age30','A_AgeInt','B_AgeInt',\n",
    "            'A_Elo_Overall','B_Elo_Overall','A_Elo_Surface','B_Elo_Surface',\n",
    "            'best_of']\n",
    "for c in num_cols:\n",
    "    miss = df[c].isna().astype('int32')\n",
    "    df[f'{c}_isna'] = miss\n",
    "    df[c] = df[c].fillna(df[c].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "31659584",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ids = df['player_A_id'].astype('int64').values\n",
    "B_ids = df['player_B_id'].astype('int64').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2baf882",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players = np.unique(np.concatenate([A_ids, B_ids]))\n",
    "pid2idx = {pid:i for i,pid in enumerate(all_players)}\n",
    "n_nodes = len(all_players)\n",
    "A_idx = np.array([pid2idx[p] for p in A_ids], dtype='int64')\n",
    "B_idx = np.array([pid2idx[p] for p in B_ids], dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbdbde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_edge = pd.concat([surf_ohe, level_ohe, df[['best_of']].astype('float32')], axis=1).astype('float32').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54d589f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dyn_cols = ['player_A_ht','player_A_rank','player_A_rank_points',\n",
    "              'A_Glicko1_Rating','A_Glicko1_Surface_Rating',\n",
    "              'A_H2H_Streak','A_H2H_LevelWeighted_Wins',\n",
    "              'A_Age30','A_AgeInt','A_Elo_Overall','A_Elo_Surface'] + \\\n",
    "             [c for c in df.columns if c.endswith('_isna') and c.startswith('player_A_') or c.startswith('A_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12ddd1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_dyn_cols = ['player_B_ht','player_B_rank','player_B_rank_points',\n",
    "              'B_Glicko1_Rating','B_Glicko1_Surface_Rating',\n",
    "              'B_H2H_Streak','B_H2H_LevelWeighted_Wins',\n",
    "              'B_Age30','B_AgeInt','B_Elo_Overall','B_Elo_Surface'] + \\\n",
    "             [c for c in df.columns if c.endswith('_isna') and c.startswith('player_B_') or c.startswith('B_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b1043cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_dyn = df[A_dyn_cols].astype('float32').values\n",
    "B_dyn = df[B_dyn_cols].astype('float32').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c81e361",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hand = Ahand_ohe.astype('float32').values\n",
    "B_hand = Bhand_ohe.astype('float32').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "720c674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_inp = np.concatenate([A_dyn, A_hand], axis=1).astype('float32')\n",
    "B_inp = np.concatenate([B_dyn, B_hand], axis=1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b91bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, va, te = idx_train, idx_val, idx_test\n",
    "data_train = (A_idx[tr], B_idx[tr], A_inp[tr], B_inp[tr], X_edge[tr], y[tr])\n",
    "data_val   = (A_idx[va], B_idx[va], A_inp[va], B_inp[va], X_edge[va], y[va])\n",
    "data_test  = (A_idx[te], B_idx[te], A_inp[te], B_inp[te], X_edge[te], y[te])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00842cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_A = StandardScaler().fit(A_inp[tr])\n",
    "sc_B = StandardScaler().fit(B_inp[tr])\n",
    "sc_E = StandardScaler().fit(X_edge[tr])\n",
    "\n",
    "A_inp = sc_A.transform(A_inp)\n",
    "B_inp = sc_B.transform(B_inp)\n",
    "X_edge = sc_E.transform(X_edge)\n",
    "\n",
    "data_train = (A_idx[tr], B_idx[tr], A_inp[tr], B_inp[tr], X_edge[tr], y[tr])\n",
    "data_val   = (A_idx[va], B_idx[va], A_inp[va], B_inp[va], X_edge[va], y[va])\n",
    "data_test  = (A_idx[te], B_idx[te], A_inp[te], B_inp[te], X_edge[te], y[te])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "051f461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "src_tr = A_idx[tr]\n",
    "dst_tr = B_idx[tr]\n",
    "# Use an undirected graph to share signal both ways; you can try directed as well.\n",
    "rows = np.concatenate([src_tr, dst_tr])\n",
    "cols = np.concatenate([dst_tr, src_tr])\n",
    "vals = np.ones_like(rows, dtype='float32')\n",
    "A_csr = sp.csr_matrix((vals, (rows, cols)), shape=(n_nodes, n_nodes))\n",
    "\n",
    "# Row-normalize for mean aggregation\n",
    "deg = np.asarray(A_csr.sum(axis=1)).ravel()\n",
    "deg[deg == 0] = 1.0\n",
    "D_inv = sp.diags(1.0 / deg)\n",
    "A_norm = D_inv @ A_csr  # simple mean aggregator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f6812b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def scipy_csr_to_tf_sparse(mat):\n",
    "    mat = mat.tocoo().astype(np.float32)\n",
    "    idx = np.stack([mat.row, mat.col], axis=1)\n",
    "    return tf.sparse.SparseTensor(indices=idx, values=mat.data, dense_shape=mat.shape)\n",
    "\n",
    "A_tf = scipy_csr_to_tf_sparse(A_norm)\n",
    "A_tf = tf.sparse.reorder(A_tf)\n",
    "\n",
    "class SimpleMPN(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_nodes, emb_dim=64, mp_hidden=64, mp_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(n_nodes, emb_dim)\n",
    "        self.ws_self = [tf.keras.layers.Dense(mp_hidden, use_bias=True) for _ in range(mp_layers)]\n",
    "        self.ws_nei  = [tf.keras.layers.Dense(mp_hidden, use_bias=False) for _ in range(mp_layers)]\n",
    "        self.act = tf.keras.layers.ReLU()\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "        self.out_dim = mp_hidden\n",
    "\n",
    "    def call(self, A_sparse, training=False):\n",
    "        H = self.emb(tf.range(n_nodes))  # (N, emb_dim)\n",
    "        for W_self, W_nei in zip(self.ws_self, self.ws_nei):\n",
    "            neigh = tf.sparse.sparse_dense_matmul(A_sparse, H)  # mean aggregated neighbors\n",
    "            H = self.act(W_self(H) + W_nei(neigh))\n",
    "            H = self.drop(H, training=training)\n",
    "        return H  # (N, out_dim)\n",
    "\n",
    "class EdgeGNN(tf.keras.Model):\n",
    "    def __init__(self, n_nodes, emb_dim=64, mp_hidden=64, mp_layers=2, mlp_hidden=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mp = SimpleMPN(n_nodes, emb_dim=emb_dim, mp_hidden=mp_hidden, mp_layers=mp_layers, dropout=dropout)\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "        self.edge_mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(mlp_hidden, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(mlp_hidden, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        src, dst, A_feat, B_feat, E_feat, A_sparse = inputs\n",
    "        H = self.mp(A_sparse, training=training)  # (N, D)\n",
    "        h_src = tf.gather(H, src)\n",
    "        h_dst = tf.gather(H, dst)\n",
    "        z = tf.concat([h_src, h_dst, h_src - h_dst, h_src * h_dst, A_feat, B_feat, E_feat], axis=-1)\n",
    "        z = self.drop(z, training=training)\n",
    "        return self.edge_mlp(z, training=training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cfb83c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1484: UserWarning: Layer 'edge_gnn_1' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 2640 (of type <class 'int'>)''\n",
      "  warnings.warn(\n",
      "c:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py:421: UserWarning: `build()` was called on layer 'edge_gnn_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling EdgeGNN.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 2640 (of type <class 'int'>)\u001b[0m\n\nArguments received by EdgeGNN.call():\n  • inputs=('tf.Tensor(shape=(None,), dtype=int64)', 'tf.Tensor(shape=(None,), dtype=int64)', 'tf.Tensor(shape=(None, 35), dtype=float32)', 'tf.Tensor(shape=(None, 35), dtype=float32)', 'tf.Tensor(shape=(None, 12), dtype=float32)', 'tf.Tensor(shape=(2640, 2640), dtype=float32)')\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     26\u001b[39m model = EdgeGNN(n_nodes=n_nodes, emb_dim=\u001b[32m256\u001b[39m, mp_hidden=\u001b[32m256\u001b[39m, mp_layers=\u001b[32m3\u001b[39m, mlp_hidden=\u001b[32m512\u001b[39m, dropout=\u001b[32m0.1\u001b[39m)\n\u001b[32m     27\u001b[39m model.compile(optimizer=tf.keras.optimizers.Adam(\u001b[32m1e-3\u001b[39m),\n\u001b[32m     28\u001b[39m               loss=\u001b[33m'\u001b[39m\u001b[33mbinary_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     29\u001b[39m               metrics=[tf.keras.metrics.AUC(name=\u001b[33m'\u001b[39m\u001b[33mAUC\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m model.evaluate(ds_te, verbose=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mEdgeGNN.call\u001b[39m\u001b[34m(self, inputs, training)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     74\u001b[39m     src, dst, A_feat, B_feat, E_feat, A_sparse = inputs\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     H = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     h_src = tf.gather(H, src)\n\u001b[32m     77\u001b[39m     h_dst = tf.gather(H, dst)\n",
      "\u001b[31mValueError\u001b[39m: Exception encountered when calling EdgeGNN.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: 2640 (of type <class 'int'>)\u001b[0m\n\nArguments received by EdgeGNN.call():\n  • inputs=('tf.Tensor(shape=(None,), dtype=int64)', 'tf.Tensor(shape=(None,), dtype=int64)', 'tf.Tensor(shape=(None, 35), dtype=float32)', 'tf.Tensor(shape=(None, 35), dtype=float32)', 'tf.Tensor(shape=(None, 12), dtype=float32)', 'tf.Tensor(shape=(2640, 2640), dtype=float32)')\n  • training=True"
     ]
    }
   ],
   "source": [
    "def make_ds(Ai, Bi, Af, Bf, Ef, y, batch=4096, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((\n",
    "        {'src': Ai, 'dst': Bi, 'Af': Af, 'Bf': Bf, 'Ef': Ef}, y.astype('float32')\n",
    "    ))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(200_000, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr = data_train\n",
    "Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va = data_val\n",
    "Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te = data_test\n",
    "\n",
    "ds_tr = make_ds(Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr, batch=4096, shuffle=True)\n",
    "ds_va = make_ds(Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va, batch=8192, shuffle=False)\n",
    "ds_te = make_ds(Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te, batch=8192, shuffle=False)\n",
    "\n",
    "def attach_adj(ds):\n",
    "    def _map(features, label):\n",
    "        return ((features['src'], features['dst'], features['Af'], features['Bf'], features['Ef'], A_tf), label)\n",
    "    return ds.map(_map)\n",
    "\n",
    "ds_tr = attach_adj(ds_tr); ds_va = attach_adj(ds_va); ds_te = attach_adj(ds_te)\n",
    "\n",
    "# Instantiate and train\n",
    "model = EdgeGNN(n_nodes=n_nodes, emb_dim=256, mp_hidden=256, mp_layers=3, mlp_hidden=512, dropout=0.1)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[tf.keras.metrics.AUC(name='AUC'), 'accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(ds_tr, validation_data=ds_va, epochs=100, verbose=2)\n",
    "\n",
    "print(\"Test:\")\n",
    "model.evaluate(ds_te, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "efa1a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "15/15 - 3s - 180ms/step - accuracy: 0.6639 - log_loss: 0.6142 - loss: 0.6142 - val_accuracy: 0.6527 - val_log_loss: 0.6309 - val_loss: 0.6309\n",
      "Epoch 2/20\n",
      "15/15 - 1s - 46ms/step - accuracy: 0.6843 - log_loss: 0.5861 - loss: 0.5861 - val_accuracy: 0.6549 - val_log_loss: 0.6144 - val_loss: 0.6144\n",
      "Epoch 3/20\n",
      "15/15 - 1s - 52ms/step - accuracy: 0.6907 - log_loss: 0.5784 - loss: 0.5784 - val_accuracy: 0.6516 - val_log_loss: 0.6225 - val_loss: 0.6225\n",
      "Epoch 4/20\n",
      "15/15 - 1s - 46ms/step - accuracy: 0.6972 - log_loss: 0.5708 - loss: 0.5708 - val_accuracy: 0.6457 - val_log_loss: 0.6295 - val_loss: 0.6295\n",
      "Epoch 5/20\n",
      "15/15 - 1s - 40ms/step - accuracy: 0.7021 - log_loss: 0.5629 - loss: 0.5629 - val_accuracy: 0.6435 - val_log_loss: 0.6406 - val_loss: 0.6406\n",
      "Epoch 6/20\n",
      "15/15 - 1s - 40ms/step - accuracy: 0.7047 - log_loss: 0.5561 - loss: 0.5561 - val_accuracy: 0.6479 - val_log_loss: 0.6452 - val_loss: 0.6452\n",
      "Epoch 7/20\n",
      "15/15 - 1s - 49ms/step - accuracy: 0.7090 - log_loss: 0.5505 - loss: 0.5505 - val_accuracy: 0.6376 - val_log_loss: 0.6602 - val_loss: 0.6602\n",
      "Epoch 8/20\n",
      "15/15 - 1s - 41ms/step - accuracy: 0.7135 - log_loss: 0.5446 - loss: 0.5446 - val_accuracy: 0.6362 - val_log_loss: 0.6710 - val_loss: 0.6710\n",
      "Epoch 9/20\n",
      "15/15 - 1s - 40ms/step - accuracy: 0.7163 - log_loss: 0.5391 - loss: 0.5391 - val_accuracy: 0.6365 - val_log_loss: 0.6786 - val_loss: 0.6786\n",
      "Epoch 10/20\n",
      "15/15 - 1s - 41ms/step - accuracy: 0.7214 - log_loss: 0.5339 - loss: 0.5339 - val_accuracy: 0.6395 - val_log_loss: 0.6891 - val_loss: 0.6891\n",
      "Epoch 11/20\n",
      "15/15 - 1s - 42ms/step - accuracy: 0.7242 - log_loss: 0.5278 - loss: 0.5278 - val_accuracy: 0.6329 - val_log_loss: 0.6965 - val_loss: 0.6965\n",
      "Epoch 12/20\n",
      "15/15 - 1s - 39ms/step - accuracy: 0.7270 - log_loss: 0.5232 - loss: 0.5232 - val_accuracy: 0.6343 - val_log_loss: 0.7102 - val_loss: 0.7102\n",
      "Epoch 13/20\n",
      "15/15 - 1s - 40ms/step - accuracy: 0.7326 - log_loss: 0.5167 - loss: 0.5167 - val_accuracy: 0.6318 - val_log_loss: 0.7221 - val_loss: 0.7221\n",
      "Epoch 14/20\n",
      "15/15 - 1s - 38ms/step - accuracy: 0.7352 - log_loss: 0.5130 - loss: 0.5130 - val_accuracy: 0.6292 - val_log_loss: 0.7228 - val_loss: 0.7228\n",
      "Epoch 15/20\n",
      "15/15 - 1s - 39ms/step - accuracy: 0.7404 - log_loss: 0.5058 - loss: 0.5058 - val_accuracy: 0.6295 - val_log_loss: 0.7317 - val_loss: 0.7317\n",
      "Epoch 16/20\n",
      "15/15 - 1s - 39ms/step - accuracy: 0.7426 - log_loss: 0.5007 - loss: 0.5007 - val_accuracy: 0.6189 - val_log_loss: 0.7538 - val_loss: 0.7538\n",
      "Epoch 17/20\n",
      "15/15 - 1s - 39ms/step - accuracy: 0.7482 - log_loss: 0.4939 - loss: 0.4939 - val_accuracy: 0.6193 - val_log_loss: 0.7696 - val_loss: 0.7696\n",
      "Epoch 18/20\n",
      "15/15 - 1s - 41ms/step - accuracy: 0.7520 - log_loss: 0.4886 - loss: 0.4886 - val_accuracy: 0.6336 - val_log_loss: 0.7778 - val_loss: 0.7778\n",
      "Epoch 19/20\n",
      "15/15 - 1s - 40ms/step - accuracy: 0.7583 - log_loss: 0.4818 - loss: 0.4818 - val_accuracy: 0.6196 - val_log_loss: 0.8065 - val_loss: 0.8065\n",
      "Epoch 20/20\n",
      "15/15 - 1s - 42ms/step - accuracy: 0.7601 - log_loss: 0.4780 - loss: 0.4780 - val_accuracy: 0.6207 - val_log_loss: 0.8022 - val_loss: 0.8022\n",
      "Test evaluation:\n",
      "2/2 - 0s - 36ms/step - accuracy: 0.5876 - log_loss: 1.0458 - loss: 1.0458\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Tennis Match GNN — End-to-end, single script\n",
    "# Optimizes log loss (binary cross-entropy)\n",
    "# ============================================\n",
    "\n",
    "# ---- Imports ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# 0) Load your DataFrame `df` with the 31 columns you shared\n",
    "#    If you already have it in memory, comment the next two lines.\n",
    "# ============================================\n",
    "df = pd.read_csv(\"df_gnn.csv\")  # Ensure it has the exact schema you shared\n",
    "\n",
    "# ============================================\n",
    "# 1) Chronological split and basic preprocessing (leakage-safe)\n",
    "# ============================================\n",
    "assert {'player_A_id','player_B_id','tourney_date','target'}.issubset(df.columns)\n",
    "\n",
    "# Convert date and sort\n",
    "df['date'] = pd.to_datetime(df['tourney_date'].astype(str), format='%Y%m%d')\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Splits: adjust as needed\n",
    "cut_val  = np.datetime64('2019-01-01')\n",
    "cut_test = np.datetime64('2020-01-01')\n",
    "dates = df['date'].values\n",
    "idx_train = np.where(dates <  cut_val)[0]\n",
    "idx_val   = np.where((dates >= cut_val) & (dates < cut_test))[0]\n",
    "idx_test  = np.where(dates >= cut_test)[0]\n",
    "\n",
    "y = df['target'].astype('int32').values\n",
    "\n",
    "# Categorical encodings\n",
    "for col in ['surface', 'tourney_level', 'player_A_hand', 'player_B_hand']:\n",
    "    df[col] = df[col].fillna('UNK').astype('category')\n",
    "\n",
    "surf_ohe   = pd.get_dummies(df['surface'], prefix='surf', drop_first=False)\n",
    "level_ohe  = pd.get_dummies(df['tourney_level'], prefix='lvl', drop_first=False)\n",
    "Ahand_ohe  = pd.get_dummies(df['player_A_hand'], prefix='Ah', drop_first=False)\n",
    "Bhand_ohe  = pd.get_dummies(df['player_B_hand'], prefix='Bh', drop_first=False)\n",
    "\n",
    "# Numeric columns with potential missingness (per-endpoint + context)\n",
    "A_num = [\n",
    "    'player_A_ht','player_A_rank','player_A_rank_points',\n",
    "    'A_Glicko1_Rating','A_Glicko1_Surface_Rating',\n",
    "    'A_H2H_Streak','A_H2H_LevelWeighted_Wins',\n",
    "    'A_Age30','A_AgeInt','A_Elo_Overall','A_Elo_Surface'\n",
    "]\n",
    "B_num = [\n",
    "    'player_B_ht','player_B_rank','player_B_rank_points',\n",
    "    'B_Glicko1_Rating','B_Glicko1_Surface_Rating',\n",
    "    'B_H2H_Streak','B_H2H_LevelWeighted_Wins',\n",
    "    'B_Age30','B_AgeInt','B_Elo_Overall','B_Elo_Surface'\n",
    "]\n",
    "edge_num = ['best_of']\n",
    "\n",
    "for c in A_num + B_num + edge_num:\n",
    "    miss = df[c].isna().astype('int32')\n",
    "    df[f'{c}_isna'] = miss\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "# Build per-endpoint dynamic features (numeric + hand one-hot + missing flags)\n",
    "A_miss = [f'{c}_isna' for c in A_num]\n",
    "B_miss = [f'{c}_isna' for c in B_num]\n",
    "\n",
    "A_dyn = df[A_num + A_miss].astype('float32').values\n",
    "B_dyn = df[B_num + B_miss].astype('float32').values\n",
    "A_hand = Ahand_ohe.astype('float32').values\n",
    "B_hand = Bhand_ohe.astype('float32').values\n",
    "\n",
    "A_inp = np.concatenate([A_dyn, A_hand], axis=1).astype('float32')\n",
    "B_inp = np.concatenate([B_dyn, B_hand], axis=1).astype('float32')\n",
    "\n",
    "# Edge/context features: surface/level OHE + numeric + its missing flag\n",
    "edge_feats = pd.concat([surf_ohe, level_ohe, df[edge_num], df[[f'{c}_isna' for c in edge_num]]], axis=1)\n",
    "X_edge = edge_feats.astype('float32').values\n",
    "\n",
    "# Map player ids to contiguous indices\n",
    "A_ids = df['player_A_id'].astype('int64').values\n",
    "B_ids = df['player_B_id'].astype('int64').values\n",
    "all_players = np.unique(np.concatenate([A_ids, B_ids]))\n",
    "pid2idx = {pid:i for i,pid in enumerate(all_players)}\n",
    "n_nodes = len(all_players)\n",
    "A_idx = np.array([pid2idx[p] for p in A_ids], dtype='int64')\n",
    "B_idx = np.array([pid2idx[p] for p in B_ids], dtype='int64')\n",
    "\n",
    "# Standardize continuous features using training stats only\n",
    "tr = idx_train; va = idx_val; te = idx_test\n",
    "\n",
    "sc_A = StandardScaler().fit(A_inp[tr])\n",
    "sc_B = StandardScaler().fit(B_inp[tr])\n",
    "sc_E = StandardScaler().fit(X_edge[tr])\n",
    "\n",
    "A_inp = sc_A.transform(A_inp).astype('float32')\n",
    "B_inp = sc_B.transform(B_inp).astype('float32')\n",
    "X_edge = sc_E.transform(X_edge).astype('float32')\n",
    "\n",
    "# Final split packs\n",
    "data_train = (A_idx[tr], B_idx[tr], A_inp[tr], B_inp[tr], X_edge[tr], y[tr])\n",
    "data_val   = (A_idx[va], B_idx[va], A_inp[va], B_inp[va], X_edge[va], y[va])\n",
    "data_test  = (A_idx[te], B_idx[te], A_inp[te], B_inp[te], X_edge[te], y[te])\n",
    "\n",
    "# ============================================\n",
    "# 2) Build train adjacency (train edges only), row-normalized\n",
    "# ============================================\n",
    "def build_norm_adj_from_edges(src_idx, dst_idx, n_nodes, undirected=True):\n",
    "    if undirected:\n",
    "        rows = np.concatenate([src_idx, dst_idx])\n",
    "        cols = np.concatenate([dst_idx, src_idx])\n",
    "    else:\n",
    "        rows, cols = src_idx, dst_idx\n",
    "    vals = np.ones_like(rows, dtype=np.float32)\n",
    "    A = sp.csr_matrix((vals, (rows, cols)), shape=(n_nodes, n_nodes))\n",
    "    deg = np.asarray(A.sum(axis=1)).ravel()\n",
    "    deg[deg == 0] = 1.0\n",
    "    D_inv = sp.diags(1.0 / deg)\n",
    "    return D_inv @ A\n",
    "\n",
    "src_tr = A_idx[tr]; dst_tr = B_idx[tr]\n",
    "A_norm = build_norm_adj_from_edges(src_tr, dst_tr, n_nodes=n_nodes, undirected=True)\n",
    "\n",
    "def scipy_csr_to_tf_sparse(mat):\n",
    "    mat = mat.tocoo().astype(np.float32)\n",
    "    idx = np.stack([mat.row, mat.col], axis=1)\n",
    "    st = tf.sparse.SparseTensor(indices=idx, values=mat.data, dense_shape=mat.shape)\n",
    "    return tf.sparse.reorder(st)\n",
    "\n",
    "A_tf = scipy_csr_to_tf_sparse(A_norm)\n",
    "\n",
    "# ============================================\n",
    "# 3) tf.data pipelines (no adjacency threaded)\n",
    "# ============================================\n",
    "def make_ds(Ai, Bi, Af, Bf, Ef, y, batch=4096, shuffle=True):\n",
    "    X = (Ai, Bi, Af.astype('float32'), Bf.astype('float32'), Ef.astype('float32'))\n",
    "    y2 = y.astype('float32').reshape(-1, 1)  # match model output shape (B, 1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y2))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(min(len(Ai), 200_000), reshuffle_each_iteration=True)\n",
    "    return ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr = data_train\n",
    "Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va = data_val\n",
    "Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te = data_test\n",
    "\n",
    "ds_tr = make_ds(Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr, batch=4096, shuffle=True)\n",
    "ds_va = make_ds(Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va, batch=8192, shuffle=False)\n",
    "ds_te = make_ds(Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te, batch=8192, shuffle=False)\n",
    "\n",
    "# ============================================\n",
    "# 4) Model: message passing + edge classifier (log-loss oriented)\n",
    "# ============================================\n",
    "class SimpleMPN(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_nodes, emb_dim=64, mp_hidden=64, mp_layers=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.emb = tf.keras.layers.Embedding(n_nodes, emb_dim)\n",
    "        self.ws_self = [tf.keras.layers.Dense(mp_hidden, use_bias=True) for _ in range(mp_layers)]\n",
    "        self.ws_nei  = [tf.keras.layers.Dense(mp_hidden, use_bias=False) for _ in range(mp_layers)]\n",
    "        self.act = tf.keras.layers.ReLU()\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "        self.out_dim = mp_hidden if mp_layers > 0 else emb_dim\n",
    "        self.mp_layers = mp_layers\n",
    "\n",
    "    def call(self, A_sparse, training=False):\n",
    "        H = self.emb(tf.range(self.n_nodes))\n",
    "        if self.mp_layers == 0:\n",
    "            return H\n",
    "        for W_self, W_nei in zip(self.ws_self, self.ws_nei):\n",
    "            neigh = tf.sparse.sparse_dense_matmul(A_sparse, H)  # mean-aggregated neighbors\n",
    "            H = self.act(W_self(H) + W_nei(neigh))\n",
    "            H = self.drop(H, training=training)\n",
    "        return H\n",
    "\n",
    "class EdgeGNN(tf.keras.Model):\n",
    "    def __init__(self, n_nodes, A_sparse, emb_dim=64, mp_hidden=64, mp_layers=2, mlp_hidden=256, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.A_sparse = A_sparse\n",
    "        self.mp = SimpleMPN(n_nodes, emb_dim=emb_dim, mp_hidden=mp_hidden, mp_layers=mp_layers, dropout=dropout)\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "        self.edge_mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(mlp_hidden, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(mlp_hidden, activation='relu'),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        src, dst, A_feat, B_feat, E_feat = inputs\n",
    "        H = self.mp(self.A_sparse, training=training)\n",
    "        h_src = tf.gather(H, src)\n",
    "        h_dst = tf.gather(H, dst)\n",
    "        z = tf.concat([h_src, h_dst, h_src - h_dst, h_src * h_dst, A_feat, B_feat, E_feat], axis=-1)\n",
    "        z = self.drop(z, training=training)\n",
    "        logits = self.edge_mlp(z, training=training)   # (B, 1)\n",
    "        return logits\n",
    "    \n",
    "def compile_for_logloss(model, lr=1e-3):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.BinaryCrossentropy(name='log_loss', from_logits=False),\n",
    "            tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5),\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Instantiate, compile, train, evaluate\n",
    "tf.keras.backend.clear_session()\n",
    "model = EdgeGNN(n_nodes=n_nodes, A_sparse=A_tf, emb_dim=64, mp_hidden=64, mp_layers=2, mlp_hidden=256, dropout=0.0)\n",
    "compile_for_logloss(model, lr=1e-3)\n",
    "\n",
    "history = model.fit(ds_tr, validation_data=ds_va, epochs=20, verbose=2)\n",
    "print(\"Test evaluation:\")\n",
    "model.evaluate(ds_te, verbose=2)\n",
    "\n",
    "# ============================================\n",
    "# 5) Double-descent experiments (log-loss)\n",
    "# ============================================\n",
    "def capacity_sweep(\n",
    "    data_train, data_val, data_test, A_tf,\n",
    "    widths=(8,16,32,64,128,256,512,1024),\n",
    "    emb_dims=(16,64),\n",
    "    mp_layers_list=(0,1,2,3),\n",
    "    mlp_multipliers=(1,),\n",
    "    epochs=40,\n",
    "    lr=1e-3,\n",
    "    seed=7\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr = data_train\n",
    "    Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va = data_val\n",
    "    Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te = data_test\n",
    "\n",
    "    ds_tr = make_ds(Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr, batch=4096, shuffle=True)\n",
    "    ds_va = make_ds(Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va, batch=8192, shuffle=False)\n",
    "    ds_te = make_ds(Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te, batch=8192, shuffle=False)\n",
    "\n",
    "    rows = []\n",
    "    for emb_dim in emb_dims:\n",
    "        for w in widths:\n",
    "            for mp_layers in mp_layers_list:\n",
    "                for mult in mlp_multipliers:\n",
    "                    mlp_hidden = max(4, int(w * mult))\n",
    "                    tf.keras.backend.clear_session()\n",
    "                    m = EdgeGNN(n_nodes=n_nodes, A_sparse=A_tf,\n",
    "                                emb_dim=emb_dim, mp_hidden=w, mp_layers=mp_layers,\n",
    "                                mlp_hidden=mlp_hidden, dropout=0.0)\n",
    "                    compile_for_logloss(m, lr=lr)\n",
    "                    m.fit(ds_tr, epochs=epochs, verbose=0, validation_data=ds_va)\n",
    "                    tr_loss, tr_log, tr_acc = m.evaluate(ds_tr, verbose=0)\n",
    "                    va_loss, va_log, va_acc = m.evaluate(ds_va, verbose=0)\n",
    "                    te_loss, te_log, te_acc = m.evaluate(ds_te, verbose=0)\n",
    "                    rows.append({\n",
    "                        'emb_dim': emb_dim, 'width': w, 'mp_layers': mp_layers, 'mlp_hidden': mlp_hidden,\n",
    "                        'train_log_loss': float(tr_loss), 'val_log_loss': float(va_loss), 'test_log_loss': float(te_loss),\n",
    "                        'train_acc': float(tr_acc), 'val_acc': float(va_acc), 'test_acc': float(te_acc)\n",
    "                    })\n",
    "                    print(f\"[cap] emb={emb_dim} w={w} L={mp_layers} mlp={mlp_hidden} -> test_log={te_loss:.4f}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def size_sweep(\n",
    "    data_train, data_val, data_test,\n",
    "    fractions=(0.1,0.2,0.4,0.6,0.8,1.0),\n",
    "    emb_dim=64, mp_hidden=256, mp_layers=2, mlp_hidden=512,\n",
    "    epochs=40, lr=1e-3, seed=13\n",
    "):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr = data_train\n",
    "    Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va = data_val\n",
    "    Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te = data_test\n",
    "\n",
    "    rows = []\n",
    "    n_tr = len(Ai_tr)\n",
    "    for frac in fractions:\n",
    "        k = max(1000, int(n_tr * frac))\n",
    "        rng = np.random.RandomState(seed)\n",
    "        idx = rng.choice(n_tr, size=k, replace=False)\n",
    "\n",
    "        Ai_sub, Bi_sub = Ai_tr[idx], Bi_tr[idx]\n",
    "        Af_sub, Bf_sub, Ef_sub, y_sub = Af_tr[idx], Bf_tr[idx], Ef_tr[idx], y_tr[idx]\n",
    "\n",
    "        # Rebuild adjacency from the subsampled training edges\n",
    "        A_sub = build_norm_adj_from_edges(Ai_sub, Bi_sub, n_nodes=n_nodes, undirected=True)\n",
    "        A_sub_tf = scipy_csr_to_tf_sparse(A_sub)\n",
    "\n",
    "        ds_tr = make_ds(Ai_sub, Bi_sub, Af_sub, Bf_sub, Ef_sub, y_sub, batch=4096, shuffle=True)\n",
    "        ds_va = make_ds(Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va, batch=8192, shuffle=False)\n",
    "        ds_te = make_ds(Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te, batch=8192, shuffle=False)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        m = EdgeGNN(n_nodes=n_nodes, A_sparse=A_sub_tf,\n",
    "                    emb_dim=emb_dim, mp_hidden=mp_hidden, mp_layers=mp_layers, mlp_hidden=mlp_hidden, dropout=0.0)\n",
    "        compile_for_logloss(m, lr=lr)\n",
    "        m.fit(ds_tr, epochs=epochs, verbose=0, validation_data=ds_va)\n",
    "        tr_loss, tr_log, tr_acc = m.evaluate(ds_tr, verbose=0)\n",
    "        va_loss, va_log, va_acc = m.evaluate(ds_va, verbose=0)\n",
    "        te_loss, te_log, te_acc = m.evaluate(ds_te, verbose=0)\n",
    "        rows.append({\n",
    "            'fraction': float(frac), 'n_train_edges': int(k),\n",
    "            'train_log_loss': float(tr_loss), 'val_log_loss': float(va_loss), 'test_log_loss': float(te_loss),\n",
    "            'train_acc': float(tr_acc), 'val_acc': float(va_acc), 'test_acc': float(te_acc),\n",
    "            'emb_dim': emb_dim, 'mp_hidden': mp_hidden, 'mp_layers': mp_layers, 'mlp_hidden': mlp_hidden\n",
    "        })\n",
    "        print(f\"[size] frac={frac:.2f} k={k} -> test_log={te_loss:.4f}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ============================================\n",
    "# 6) Run the sweeps (optional; can be time-consuming)\n",
    "# ============================================\n",
    "# cap_results = capacity_sweep(data_train, data_val, data_test, A_tf,\n",
    "#                              widths=(8,16,32,64,128,256,512,1024),\n",
    "#                              emb_dims=(16,64),\n",
    "#                              mp_layers_list=(0,1,2,3),\n",
    "#                              mlp_multipliers=(1,),\n",
    "#                              epochs=40, lr=1e-3, seed=7)\n",
    "#\n",
    "# size_results = size_sweep(data_train, data_val, data_test,\n",
    "#                           fractions=(0.1,0.2,0.4,0.6,0.8,1.0),\n",
    "#                           emb_dim=64, mp_hidden=256, mp_layers=2, mlp_hidden=512,\n",
    "#                           epochs=40, lr=1e-3, seed=13)\n",
    "\n",
    "# Example plot for capacity (uncomment after running cap_results)\n",
    "# mask = (cap_results['mp_layers'] == 2) & (cap_results['emb_dim'] == 64)\n",
    "# sub = cap_results[mask].sort_values('width')\n",
    "# plt.figure()\n",
    "# plt.plot(sub['width'], sub['test_log_loss'], marker='o')\n",
    "# plt.xscale('log', base=2)\n",
    "# plt.xlabel('Width (log2 scale)')\n",
    "# plt.ylabel('Test log loss')\n",
    "# plt.title('Double Descent: Capacity Sweep (mp_layers=2, emb_dim=64)')\n",
    "# plt.show()\n",
    "\n",
    "# Example plot for sample size (uncomment after running size_results)\n",
    "# ssub = size_results.sort_values('n_train_edges')\n",
    "# plt.figure()\n",
    "# plt.plot(ssub['n_train_edges'], ssub['test_log_loss'], marker='o')\n",
    "# plt.xlabel('# Training edges')\n",
    "# plt.ylabel('Test log loss')\n",
    "# plt.title('Double Descent: Sample-Size Sweep')\n",
    "# plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 7) Inference helper for a future match (P(A wins))\n",
    "#    Provide pre-match A_features, B_features, edge_features in the same order used above.\n",
    "# ============================================\n",
    "def predict_match_proba(model, player_A_id, player_B_id, A_features_vec, B_features_vec, edge_features_vec):\n",
    "    src = np.array([pid2idx[player_A_id]], dtype='int64')\n",
    "    dst = np.array([pid2idx[player_B_id]], dtype='int64')\n",
    "    Af = sc_A.transform(np.asarray(A_features_vec, dtype='float32').reshape(1, -1))\n",
    "    Bf = sc_B.transform(np.asarray(B_features_vec, dtype='float32').reshape(1, -1))\n",
    "    Ef = sc_E.transform(np.asarray(edge_features_vec, dtype='float32').reshape(1, -1))\n",
    "    p = model((src, dst, Af.astype('float32'), Bf.astype('float32'), Ef.astype('float32')), training=False).numpy()[0,0]\n",
    "    return float(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04fef729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capacity_sweep_multi(\n",
    "    data_train, data_val, data_test, A_tf,\n",
    "    widths=(8,16,32,64,128,256,512,1024),\n",
    "    emb_dims=(16,64),\n",
    "    mp_layers_list=(0,1,2,3),\n",
    "    mlp_multipliers=(1,),\n",
    "    epochs=40, lr=1e-3, seeds=(1,2,3,4,5)\n",
    "):\n",
    "    Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr = data_train\n",
    "    Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va = data_val\n",
    "    Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te = data_test\n",
    "\n",
    "    rows = []\n",
    "    for emb_dim in emb_dims:\n",
    "        for w in widths:\n",
    "            for mp_layers in mp_layers_list:\n",
    "                for mult in mlp_multipliers:\n",
    "                    mlp_hidden = max(4, int(w * mult))\n",
    "                    for seed in seeds:\n",
    "                        tf.keras.utils.set_random_seed(seed)\n",
    "                        ds_tr = make_ds(Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr, batch=4096, shuffle=True)\n",
    "                        ds_va = make_ds(Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va, batch=8192, shuffle=False)\n",
    "                        ds_te = make_ds(Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te, batch=8192, shuffle=False)\n",
    "\n",
    "                        tf.keras.backend.clear_session()\n",
    "                        m = EdgeGNN(\n",
    "                            n_nodes=n_nodes, A_sparse=A_tf,\n",
    "                            emb_dim=emb_dim, mp_hidden=w, mp_layers=mp_layers,\n",
    "                            mlp_hidden=mlp_hidden, dropout=0.0\n",
    "                        )\n",
    "                        compile_for_logloss(m, lr=lr)\n",
    "                        m.fit(ds_tr, epochs=epochs, verbose=0, validation_data=ds_va)\n",
    "\n",
    "                        # evaluate returns [loss, log_loss(metric), accuracy]\n",
    "                        tr_loss, tr_log, tr_acc = m.evaluate(ds_tr, verbose=0)\n",
    "                        va_loss, va_log, va_acc = m.evaluate(ds_va, verbose=0)\n",
    "                        te_loss, te_log, te_acc = m.evaluate(ds_te, verbose=0)\n",
    "\n",
    "                        rows.append({\n",
    "                            'seed': seed, 'emb_dim': emb_dim, 'width': w, 'mp_layers': mp_layers, 'mlp_hidden': mlp_hidden,\n",
    "                            'train_log_loss': float(tr_log), 'val_log_loss': float(va_log), 'test_log_loss': float(te_log),\n",
    "                            'train_acc': float(tr_acc), 'val_acc': float(va_acc), 'test_acc': float(te_acc)\n",
    "                        })\n",
    "                        print(f\"[cap] seed={seed} emb={emb_dim} w={w} L={mp_layers} mlp={mlp_hidden} -> test_log={te_log:.4f}\")\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb7bb686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_capacity(df, emb_dim=64, mp_layers=2):\n",
    "    sub = df[(df['emb_dim']==emb_dim) & (df['mp_layers']==mp_layers)].copy()\n",
    "    g = sub.groupby('width')['test_log_loss']\n",
    "    summary = g.agg(['mean','std','count']).reset_index()\n",
    "    # 95% CI assuming normality across seeds\n",
    "    summary['sem'] = summary['std'] / np.sqrt(summary['count'].clip(lower=1))\n",
    "    summary['ci95'] = 1.96 * summary['sem']\n",
    "    # finite differences on the mean curve (log2 spacing is okay for widths)\n",
    "    summary = summary.sort_values('width')\n",
    "    diffs = np.diff(summary['mean'].values)\n",
    "    widths_arr = summary['width'].values\n",
    "    return summary, diffs, widths_arr\n",
    "\n",
    "def has_double_descent(summary):\n",
    "    # A minimal criterion: exists i<j<k with mean[i] < mean[j] > mean[k],\n",
    "    # and both rises/drops exceed the local CI bands.\n",
    "    m = summary['mean'].values\n",
    "    c = summary['ci95'].values\n",
    "    n = len(m)\n",
    "    found = False\n",
    "    where = None\n",
    "    for j in range(1, n-1):\n",
    "        if m[j] - max(m[j-1], m[j+1]) > max(c[j], c[j-1], c[j+1]):  # peak beyond uncertainty\n",
    "            if (m[j] > m[j-1]) and (m[j] > m[j+1]):\n",
    "                found = True\n",
    "                where = j\n",
    "                break\n",
    "    return found, where\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5cf5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_capacity(summary, train_summary=None, title=\"Double Descent: Capacity Sweep\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    x = summary['width'].values\n",
    "    y = summary['mean'].values\n",
    "    ci = summary['ci95'].values\n",
    "    plt.figure()\n",
    "    plt.fill_between(x, y-ci, y+ci, alpha=0.2, linewidth=0)\n",
    "    plt.plot(x, y, marker='o')\n",
    "    if train_summary is not None:\n",
    "        plt.plot(train_summary['width'].values, train_summary['mean'].values, marker='x', linestyle='--')\n",
    "    plt.xscale('log', base=2)\n",
    "    plt.xlabel('Width (log2 scale)')\n",
    "    plt.ylabel('Test log loss')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1c80ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cap] seed=1 emb=64 w=16 L=0 mlp=16 -> test_log=0.8733\n",
      "[cap] seed=2 emb=64 w=16 L=0 mlp=16 -> test_log=0.8516\n",
      "[cap] seed=3 emb=64 w=16 L=0 mlp=16 -> test_log=0.8477\n",
      "[cap] seed=1 emb=64 w=16 L=1 mlp=16 -> test_log=0.9070\n",
      "[cap] seed=2 emb=64 w=16 L=1 mlp=16 -> test_log=0.8384\n",
      "[cap] seed=3 emb=64 w=16 L=1 mlp=16 -> test_log=0.8024\n",
      "[cap] seed=1 emb=64 w=16 L=2 mlp=16 -> test_log=0.9061\n",
      "[cap] seed=2 emb=64 w=16 L=2 mlp=16 -> test_log=0.8674\n",
      "[cap] seed=3 emb=64 w=16 L=2 mlp=16 -> test_log=0.9470\n",
      "[cap] seed=1 emb=64 w=32 L=0 mlp=32 -> test_log=0.9376\n",
      "[cap] seed=2 emb=64 w=32 L=0 mlp=32 -> test_log=0.9008\n",
      "[cap] seed=3 emb=64 w=32 L=0 mlp=32 -> test_log=0.9584\n",
      "[cap] seed=1 emb=64 w=32 L=1 mlp=32 -> test_log=0.9206\n",
      "[cap] seed=2 emb=64 w=32 L=1 mlp=32 -> test_log=0.9514\n",
      "[cap] seed=3 emb=64 w=32 L=1 mlp=32 -> test_log=0.9101\n",
      "[cap] seed=1 emb=64 w=32 L=2 mlp=32 -> test_log=0.9872\n",
      "[cap] seed=2 emb=64 w=32 L=2 mlp=32 -> test_log=1.0192\n",
      "[cap] seed=3 emb=64 w=32 L=2 mlp=32 -> test_log=1.0913\n",
      "[cap] seed=1 emb=64 w=64 L=0 mlp=64 -> test_log=1.0638\n",
      "[cap] seed=2 emb=64 w=64 L=0 mlp=64 -> test_log=1.0370\n",
      "[cap] seed=3 emb=64 w=64 L=0 mlp=64 -> test_log=1.0306\n",
      "[cap] seed=1 emb=64 w=64 L=1 mlp=64 -> test_log=1.0681\n",
      "[cap] seed=2 emb=64 w=64 L=1 mlp=64 -> test_log=1.0475\n",
      "[cap] seed=3 emb=64 w=64 L=1 mlp=64 -> test_log=1.0789\n",
      "[cap] seed=1 emb=64 w=64 L=2 mlp=64 -> test_log=1.0882\n",
      "[cap] seed=2 emb=64 w=64 L=2 mlp=64 -> test_log=1.3511\n",
      "[cap] seed=3 emb=64 w=64 L=2 mlp=64 -> test_log=1.1233\n",
      "[cap] seed=1 emb=64 w=128 L=0 mlp=128 -> test_log=1.2264\n",
      "[cap] seed=2 emb=64 w=128 L=0 mlp=128 -> test_log=1.1840\n",
      "[cap] seed=3 emb=64 w=128 L=0 mlp=128 -> test_log=1.2296\n",
      "[cap] seed=1 emb=64 w=128 L=1 mlp=128 -> test_log=1.3437\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 211\u001b[39m\n\u001b[32m    204\u001b[39m     plt.show()\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# ---------- Example usage ----------\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# You can narrow the grids first to confirm wiring, then expand.\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Comment/uncomment as needed.\u001b[39;00m\n\u001b[32m    209\u001b[39m \n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Capacity sweep run (quick example settings)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m cap_df = \u001b[43mcapacity_sweep_multi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwidths\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43memb_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmp_layers_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmlp_multipliers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\n\u001b[32m    218\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m cap_df.to_csv(\u001b[33m\"\u001b[39m\u001b[33mcap_results_multi.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    221\u001b[39m cap_sum = summarize_capacity(cap_df, emb_dim=\u001b[32m64\u001b[39m, mp_layers=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mcapacity_sweep_multi\u001b[39m\u001b[34m(data_train, data_val, data_test, A_tf, widths, emb_dims, mp_layers_list, mlp_multipliers, epochs, lr, seeds, batch_tr, batch_eval, dropout)\u001b[39m\n\u001b[32m     43\u001b[39m m = EdgeGNN(\n\u001b[32m     44\u001b[39m     n_nodes=n_nodes, A_sparse=A_tf,\n\u001b[32m     45\u001b[39m     emb_dim=emb_dim, mp_hidden=w, mp_layers=mp_layers,\n\u001b[32m     46\u001b[39m     mlp_hidden=mlp_hidden, dropout=dropout\n\u001b[32m     47\u001b[39m )\n\u001b[32m     48\u001b[39m compile_for_logloss(m, lr=lr)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43mm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_va\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# evaluate() returns: [loss, log_loss(metric), accuracy] in our compile order\u001b[39;00m\n\u001b[32m     52\u001b[39m tr_loss, tr_log, tr_acc = m.evaluate(ds_tr, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AMON\\Documents\\GitHub\\tennis\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Double Descent — multi-seed sweeps, summaries, CI plots, detection\n",
    "# Assumes your previous cells defined:\n",
    "#   - data_train, data_val, data_test\n",
    "#   - A_tf, n_nodes\n",
    "#   - make_ds, build_norm_adj_from_edges, scipy_csr_to_tf_sparse\n",
    "#   - EdgeGNN, compile_for_logloss\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# ---------- Capacity sweep across multiple seeds ----------\n",
    "def capacity_sweep_multi(\n",
    "    data_train, data_val, data_test, A_tf,\n",
    "    widths=(8,16,32,64,128,256,512,1024),\n",
    "    emb_dims=(16,64),\n",
    "    mp_layers_list=(0,1,2,3),\n",
    "    mlp_multipliers=(1,),\n",
    "    epochs=40, lr=1e-3, seeds=(1,2,3,4,5),\n",
    "    batch_tr=4096, batch_eval=8192, dropout=0.0\n",
    "):\n",
    "    Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr = data_train\n",
    "    Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va = data_val\n",
    "    Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te = data_test\n",
    "\n",
    "    rows = []\n",
    "    for emb_dim in emb_dims:\n",
    "        for w in widths:\n",
    "            for mp_layers in mp_layers_list:\n",
    "                for mult in mlp_multipliers:\n",
    "                    mlp_hidden = max(4, int(w * mult))\n",
    "                    for seed in seeds:\n",
    "                        tf.keras.utils.set_random_seed(seed)\n",
    "\n",
    "                        ds_tr = make_ds(Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr, batch=batch_tr, shuffle=True)\n",
    "                        ds_va = make_ds(Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va, batch=batch_eval, shuffle=False)\n",
    "                        ds_te = make_ds(Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te, batch=batch_eval, shuffle=False)\n",
    "\n",
    "                        tf.keras.backend.clear_session()\n",
    "                        m = EdgeGNN(\n",
    "                            n_nodes=n_nodes, A_sparse=A_tf,\n",
    "                            emb_dim=emb_dim, mp_hidden=w, mp_layers=mp_layers,\n",
    "                            mlp_hidden=mlp_hidden, dropout=dropout\n",
    "                        )\n",
    "                        compile_for_logloss(m, lr=lr)\n",
    "                        m.fit(ds_tr, epochs=epochs, verbose=0, validation_data=ds_va)\n",
    "\n",
    "                        # evaluate() returns: [loss, log_loss(metric), accuracy] in our compile order\n",
    "                        tr_loss, tr_log, tr_acc = m.evaluate(ds_tr, verbose=0)\n",
    "                        va_loss, va_log, va_acc = m.evaluate(ds_va, verbose=0)\n",
    "                        te_loss, te_log, te_acc = m.evaluate(ds_te, verbose=0)\n",
    "\n",
    "                        rows.append({\n",
    "                            'seed': seed, 'emb_dim': emb_dim, 'width': w, 'mp_layers': mp_layers,\n",
    "                            'mlp_hidden': mlp_hidden, 'dropout': dropout, 'epochs': epochs, 'lr': lr,\n",
    "                            'train_log_loss': float(tr_log), 'val_log_loss': float(va_log), 'test_log_loss': float(te_log),\n",
    "                            'train_acc': float(tr_acc), 'val_acc': float(va_acc), 'test_acc': float(te_acc)\n",
    "                        })\n",
    "                        print(f\"[cap] seed={seed} emb={emb_dim} w={w} L={mp_layers} mlp={mlp_hidden} -> test_log={te_log:.4f}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- Summaries, CI, and detection for capacity ----------\n",
    "def summarize_capacity(df, emb_dim=64, mp_layers=2):\n",
    "    sub = df[(df['emb_dim']==emb_dim) & (df['mp_layers']==mp_layers)].copy()\n",
    "    sub = sub.sort_values(['width', 'seed'])\n",
    "    g_test = sub.groupby('width')['test_log_loss']\n",
    "    g_train = sub.groupby('width')['train_log_loss']\n",
    "\n",
    "    def _agg(g):\n",
    "        out = g.agg(['mean','std','count']).reset_index()\n",
    "        out['sem'] = out['std'] / np.sqrt(out['count'].clip(lower=1))\n",
    "        out['ci95'] = 1.96 * out['sem']\n",
    "        return out\n",
    "\n",
    "    test_summary = _agg(g_test)\n",
    "    train_summary = _agg(g_train).rename(columns={'mean':'mean_train','std':'std_train','count':'count_train','sem':'sem_train','ci95':'ci95_train'})\n",
    "    merged = pd.merge(test_summary, train_summary[['width','mean_train','ci95_train']], on='width', how='left')\n",
    "    merged = merged.sort_values('width').reset_index(drop=True)\n",
    "    return merged\n",
    "\n",
    "def has_double_descent(summary_df):\n",
    "    # Minimal criterion: exists j with mean[j] > mean[j-1] and mean[j] > mean[j+1],\n",
    "    # and the peak height exceeds local uncertainty bands.\n",
    "    m = summary_df['mean'].values\n",
    "    c = summary_df['ci95'].values\n",
    "    n = len(m)\n",
    "    for j in range(1, n-1):\n",
    "        rise = m[j] - m[j-1]\n",
    "        drop = m[j] - m[j+1]\n",
    "        if rise > 0 and drop > 0:\n",
    "            # Require peak above neighbors by more than average of CIs in the triplet\n",
    "            thr = max(c[j], c[j-1], c[j+1])\n",
    "            if min(rise, drop) > thr:\n",
    "                return True, j\n",
    "    return False, None\n",
    "\n",
    "def plot_capacity(summary_df, title=\"Capacity sweep: Test log loss vs width\"):\n",
    "    x = summary_df['width'].values\n",
    "    y = summary_df['mean'].values\n",
    "    ci = summary_df['ci95'].values\n",
    "    ytr = summary_df['mean_train'].values\n",
    "    citr = summary_df.get('ci95_train', pd.Series(np.zeros_like(y))).values\n",
    "\n",
    "    plt.figure()\n",
    "    plt.fill_between(x, y-ci, y+ci, alpha=0.2, linewidth=0)\n",
    "    plt.plot(x, y, marker='o', label='Test log loss')\n",
    "    plt.fill_between(x, ytr-citr, ytr+citr, alpha=0.15, linewidth=0)\n",
    "    plt.plot(x, ytr, marker='x', linestyle='--', label='Train log loss')\n",
    "    plt.xscale('log', base=2)\n",
    "    plt.xlabel('Width (log2 scale)')\n",
    "    plt.ylabel('Log loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---------- Sample-size sweep across multiple seeds ----------\n",
    "def size_sweep_multi(\n",
    "    data_train, data_val, data_test,\n",
    "    fractions=(0.1,0.2,0.4,0.6,0.8,1.0),\n",
    "    emb_dim=64, mp_hidden=256, mp_layers=2, mlp_hidden=512,\n",
    "    epochs=40, lr=1e-3, seeds=(1,2,3,4,5),\n",
    "    batch_tr=4096, batch_eval=8192, dropout=0.0\n",
    "):\n",
    "    Ai_tr, Bi_tr, Af_tr, Bf_tr, Ef_tr, y_tr = data_train\n",
    "    Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va = data_val\n",
    "    Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te = data_test\n",
    "\n",
    "    rows = []\n",
    "    n_tr = len(Ai_tr)\n",
    "    for seed in seeds:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        for frac in fractions:\n",
    "            k = max(1000, int(n_tr * frac))\n",
    "            idx = rng.choice(n_tr, size=k, replace=False)\n",
    "\n",
    "            Ai_sub, Bi_sub = Ai_tr[idx], Bi_tr[idx]\n",
    "            Af_sub, Bf_sub, Ef_sub, y_sub = Af_tr[idx], Bf_tr[idx], Ef_tr[idx], y_tr[idx]\n",
    "\n",
    "            # Rebuild adjacency from subsampled training edges\n",
    "            A_sub = build_norm_adj_from_edges(Ai_sub, Bi_sub, n_nodes=n_nodes, undirected=True)\n",
    "            A_sub_tf = scipy_csr_to_tf_sparse(A_sub)\n",
    "\n",
    "            ds_tr = make_ds(Ai_sub, Bi_sub, Af_sub, Bf_sub, Ef_sub, y_sub, batch=batch_tr, shuffle=True)\n",
    "            ds_va = make_ds(Ai_va, Bi_va, Af_va, Bf_va, Ef_va, y_va, batch=batch_eval, shuffle=False)\n",
    "            ds_te = make_ds(Ai_te, Bi_te, Af_te, Bf_te, Ef_te, y_te, batch=batch_eval, shuffle=False)\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "            m = EdgeGNN(n_nodes=n_nodes, A_sparse=A_sub_tf,\n",
    "                        emb_dim=emb_dim, mp_hidden=mp_hidden, mp_layers=mp_layers,\n",
    "                        mlp_hidden=mlp_hidden, dropout=dropout)\n",
    "            compile_for_logloss(m, lr=lr)\n",
    "            m.fit(ds_tr, epochs=epochs, verbose=0, validation_data=ds_va)\n",
    "            tr_loss, tr_log, tr_acc = m.evaluate(ds_tr, verbose=0)\n",
    "            va_loss, va_log, va_acc = m.evaluate(ds_va, verbose=0)\n",
    "            te_loss, te_log, te_acc = m.evaluate(ds_te, verbose=0)\n",
    "\n",
    "            rows.append({\n",
    "                'seed': seed, 'fraction': float(frac), 'n_train_edges': int(k),\n",
    "                'emb_dim': emb_dim, 'mp_hidden': mp_hidden, 'mp_layers': mp_layers, 'mlp_hidden': mlp_hidden,\n",
    "                'epochs': epochs, 'lr': lr, 'dropout': dropout,\n",
    "                'train_log_loss': float(tr_log), 'val_log_loss': float(va_log), 'test_log_loss': float(te_log),\n",
    "                'train_acc': float(tr_acc), 'val_acc': float(va_acc), 'test_acc': float(te_acc),\n",
    "            })\n",
    "            print(f\"[size] seed={seed} frac={frac:.2f} k={k} -> test_log={te_log:.4f}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_size(df):\n",
    "    sub = df.copy()\n",
    "    sub = sub.sort_values(['n_train_edges', 'seed'])\n",
    "    g_test = sub.groupby('n_train_edges')['test_log_loss']\n",
    "    g_train = sub.groupby('n_train_edges')['train_log_loss']\n",
    "\n",
    "    def _agg(g):\n",
    "        out = g.agg(['mean','std','count']).reset_index()\n",
    "        out['sem'] = out['std'] / np.sqrt(out['count'].clip(lower=1))\n",
    "        out['ci95'] = 1.96 * out['sem']\n",
    "        return out\n",
    "\n",
    "    test_summary = _agg(g_test)\n",
    "    train_summary = _agg(g_train).rename(columns={'mean':'mean_train','std':'std_train','count':'count_train','sem':'sem_train','ci95':'ci95_train'})\n",
    "    merged = pd.merge(test_summary, train_summary[['n_train_edges','mean_train','ci95_train']], on='n_train_edges', how='left')\n",
    "    merged = merged.sort_values('n_train_edges').reset_index(drop=True)\n",
    "    return merged\n",
    "\n",
    "def plot_size(summary_df, title=\"Sample-size sweep: Test log loss vs # train edges\"):\n",
    "    x = summary_df['n_train_edges'].values\n",
    "    y = summary_df['mean'].values\n",
    "    ci = summary_df['ci95'].values\n",
    "    ytr = summary_df['mean_train'].values\n",
    "    citr = summary_df.get('ci95_train', pd.Series(np.zeros_like(y))).values\n",
    "\n",
    "    plt.figure()\n",
    "    plt.fill_between(x, y-ci, y+ci, alpha=0.2, linewidth=0)\n",
    "    plt.plot(x, y, marker='o', label='Test log loss')\n",
    "    plt.fill_between(x, ytr-citr, ytr+citr, alpha=0.15, linewidth=0)\n",
    "    plt.plot(x, ytr, marker='x', linestyle='--', label='Train log loss')\n",
    "    plt.xlabel('# Training edges')\n",
    "    plt.ylabel('Log loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# ---------- Example usage ----------\n",
    "# You can narrow the grids first to confirm wiring, then expand.\n",
    "# Comment/uncomment as needed.\n",
    "\n",
    "# Capacity sweep run (quick example settings)\n",
    "cap_df = capacity_sweep_multi(\n",
    "    data_train, data_val, data_test, A_tf,\n",
    "    widths=(16,32,64,128,256,512),\n",
    "    emb_dims=(64,),\n",
    "    mp_layers_list=(0,1,2),\n",
    "    mlp_multipliers=(1,),\n",
    "    epochs=30, lr=1e-3, seeds=(1,2,3), dropout=0.0\n",
    ")\n",
    "cap_df.to_csv(\"cap_results_multi.csv\", index=False)\n",
    "\n",
    "cap_sum = summarize_capacity(cap_df, emb_dim=64, mp_layers=2)\n",
    "flag, peak_idx = has_double_descent(cap_sum)\n",
    "print(f\"Double descent detected (capacity)? {flag}. Peak index: {peak_idx}.\")\n",
    "plot_capacity(cap_sum, title=\"Capacity sweep (mp_layers=2, emb_dim=64)\")\n",
    "\n",
    "# Sample-size sweep run (quick example settings)\n",
    "size_df = size_sweep_multi(\n",
    "    data_train, data_val, data_test,\n",
    "    fractions=(0.2,0.4,0.6,0.8,1.0),\n",
    "    emb_dim=64, mp_hidden=256, mp_layers=2, mlp_hidden=512,\n",
    "    epochs=30, lr=1e-3, seeds=(1,2,3), dropout=0.0\n",
    ")\n",
    "size_df.to_csv(\"size_results_multi.csv\", index=False)\n",
    "\n",
    "size_sum = summarize_size(size_df)\n",
    "# A loose check for a \"bump then drop\" as n_train_edges increases:\n",
    "# We can reuse the same detector by renaming columns\n",
    "tmp = size_sum.rename(columns={'n_train_edges':'width'})[['width','mean','ci95','mean_train','ci95_train']]\n",
    "flag_size, peak_idx_size = has_double_descent(tmp)\n",
    "print(f\"Double descent detected (sample size)? {flag_size}. Peak index: {peak_idx_size}.\")\n",
    "plot_size(size_sum, title=\"Sample-size sweep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbd1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
